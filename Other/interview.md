# 面经
## 一）. 操作系统

### 1. 进程的状态以及各个状态之间的切换
- 就绪:进程已处于准备好运行的状态，即进程已分配到除CPU外的所有必要资源后，只要再获得CPU，便可立即执行
- 执行:进程已经获得CPU，程序正在执行状态
- 阻塞:正在执行的进程由于发生某事件（如I/O请求、申请缓冲区失败等）暂时无法继续执行的状态

### 2. 进程和线程
- 进程是具有一定独立功能的程序关于某个数据集合上的一次运行活动，进程是系统进行资源分配和调度的基本单位
- 线程是进程的一个实体，是CPU调度和分派的基本单位

#### 2.1 进程和线程的关系

- 一个线程只能属于一个进程，而一个进程可以有多个线程，但至少有一个线程。
- 资源分配给进程，同一进程的所有线程共享该进程的所有资源。同一进程中的多个线程共享代码段（代码和常量），数据段（全局变量和静态变量），扩展段（堆存储）。但是每个线程拥有自己的栈段，栈段用来存放所有局部变量和临时变量
- 处理机分给线程，即真正在处理机上运行的是线程
- 线程在执行过程中，需要协作同步。不同进程的线程间要利用消息通信的方法实现同步

#### 2.2 线程与进程的区别
- 进程有自己独立的地址空间，线程没有
- 进程是系统资源分配的最小单位，线程是CPU调度的最小单位
- 进程和线程通信方式不同（线程之间的通信比较方便。同一进程下的线程共享数据（比如全局变量，静态变量），通过这些数据来通信不仅快捷而且方便，当然如何处理好这些访问的同步与互斥正是编写多线程程序的难点。而进程之间的通信只能通过进程通信的方式进行。）
- 进程上下文切换开销大，线程开销小
- 一个进程挂掉了不会影响其他进程，而线程挂掉了会影响其他线程
- 对进程操作一般开销都比较大，对线程开销小

#### 2.3 为什么进程上下文切换比线程上下文切换代价高
- 进程的步骤是：
	- 1. 切换全局目录
	- 2. 切换内核栈
	- 3. 切换硬件上下文
线程和进程的最大区别就在于地址空间，对于线程切换，第一步是不需要做的。
- 切换全局目录的性能消耗：
	- 线程上下文切换和进程上下文切换一个最主要的区别是线程的切换虚拟内存空间依然是相同的，但是进程切换是不同的。这两种上下文切换的处理都是通过操作系统内核来完成的。内核的这种切换过程伴随的是最显著的性能损耗是将寄存器中的内容切换出
	- 另外一个隐藏的损耗是上下文的切换会扰乱处理器的缓存机制。简单的说，一旦去切换上下文，处理器中所有已经缓存的内存地址一瞬间都作废了。还有一个显著的区别是当你改变虚拟内存空间的时候，处理的页表缓冲被全部刷新，这将导致内存的访问在一段时间内相当的低效。但是在线程的切换中，不会出现这个问题

### 3. 进程同步
- 临界区：通过对多线程的串行化来访问公共资源或一段代码，速度快，适合控制数据访问。
    - 优点：保证在某一时刻只有一个线程能访问数据的简便方法
    - 缺点：虽然临界区同步速度很快，但却只能用来同步本进程内的线程，而不可用来同步多个进程种的线程
- 互斥量：为协调共同对一个共享资源的单独访问而设计的，互斥量跟临界区很相似，比临界区复杂，互斥对象只有一个，只有拥有互斥对象的线程才具有访问资源的权限。
    - 优点：使用互斥不仅仅能够在同一应用程序不同线程中实现资源的安全共享，而且可以在不同应用程序的线程之间实现对资源的安全共享。
    - 缺点：1. 互斥量是可以命名的，也就是说它可以跨越进程使用。所以创建互斥量需要的资源更多，所以如果只为了在进程内部是用的话使用临界区会带来速度上的优势并能够减少资源占用量。因为互斥量是跨进程的互斥量一旦被创建，就可以通过名字打开它。 2. 通过互斥量可以指定资源被独占的方式使用，但如果有下面一种情况通过互斥量就无法处理，比如现在一位用户购买了一份三个并发访问许可的数据库系统，可以根据用户购买的访问许可数量来决定有多少个线程/进程能同时进行数据库操作，这时候如果利用互斥量就没有办法完成这个要求，信号量对象可以说是一种资源计数器。
- 信号量：为控制一个具有有限数量用户资源而设计。它允许多个线程在同一时刻访问同一资源，但是需要限制在同一时刻访问此资源的最大线程数目。互斥量是信号量的一种特殊情况，当信号量的最大资源数=1就是互斥量了。
    - 优点：适用于对Socket（套接字）程序中线程的同步。（例如，网络上的HTTP服务器要对同一时间内访问同一页面的用户数加以限制，只有不大于设定的最大用户数目的线程能够进行访问，而其他的访问企图则被挂起，只有在有用户退出对此页面的访问后才有可能进入。）
    - 缺点：1. 信号量机制必须有公共内存，不能用于分布式操作系统，这是它最大的弱点； 2.信号量机制功能强大，但使用时对信号量的操作分散， 而且难以控制，读写和维护都很困难，加重了程序员的编码负担；3.核心操作P-V分散在各用户程序的代码中，不易控制和管理，一旦错误，后果严重，且不易发现和纠正。
- 事件：用来通知线程有一些事件已发生，从而启动后继任务的开始。
    - 优点：事件对象通过通知操作的方式来保持线程的同步，并且可以实现不同进程中的线程同步操作。

### 4. 进程的通信方式有哪些
- 管道：管道是单向、先进先出的、无结构的、固定大小的字节流，它把一个进程的标准输出和另一个进程的标准输入连接在一起。写进程在管道的尾端写入数据，读进程在管道的道端读出数据。
- 信号量：信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因为，主要作为进程间及同一进程内不同线程之间的同步手段。
- 消息队列：是一个在系统内核中用来保存消息的队列，它在系统内核中是以消息链表的形式出现的。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限制等缺点。
- 共享内存：共享内存允许两个或多个进程访问同一个逻辑内存。这一段内存可以被两个或两个以上的进程映射到自身的地址空间中，一个进程写入功效内存的信息，可以被其他使用这个共享内存的进程，通过一个简单的内存读取读出，从而实现了进程间的通信。如果某个进程向共享内存写入数据，所做的改动将立即影响到可以访问同一段共享内存的任何其他进程。共享内存是最快的IPC方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制（如信号量）配合使用，来实现进程间的同步和通信
- 套接字：套接字也是一种进程间通信机制，与其它通信机制不同的是，它可用于不同机器间的进程通信

### 5. 进程调度
#### 进程调度算法
- 优先调度算法
	- 先来先服务调度算法（FCFS）：当在作业调度中采用该算法时，每次调度都是从后备作业队列中选择一个或多个最先进入该队列的作业，将它们调入内存，为它们分配资源、创建进程，然后放入就绪队列。
	- 短作业（进程）优先调度算法（SJF）：是从后备队列中选择一个或若干个估计运行时间最短的作业，将它们调入内存运行。该算法未照顾紧迫型作业。
- 高优先权优先调度算法
　　为了照顾紧迫型作业，使之在进入系统后便获得优先处理，引入了最高优先权（FPF）优先调度算法。当把该算法用于作业调度时，系统将从后备队列中选择若干个优先权最高的作业装入内存。当用于进程调度时，该算法时把处理机分配给就绪队列中优先权最高的进程。
	- 非抢占式优先权算法：在这种方式下，系统一旦把处理机分配给就绪队列中优先权最高的进程后，该进程便一直执行下去，直至完成；或因发生某事件使该进程放弃处理机时。这种调度算法主要用于批处理系统中，也可用于某些对实时性要求不严的实时系统中。
	- 抢占式优先权调度算法：在这种方式下，系统同样是把处理机分配给优先权最高的进程，使之执行。但在其执行期间，只要又出现了另一个其优先权更高的进程，进程调度就立即停止当前进程（原优先权最高的进程）的执行，重新将处理机分配给新到的优先权最高的进程。显然，这种抢占式的优先权调度算法能更好地满足紧迫作业的要求，故而常用于比较严格的实时系统中，以及对性能要求较高的批处理系统和分时系统中。
	- 高响应比优先调度算法：在批处理系统中，短作业优先算法是一种比较好的算法，其主要的不足之处是长作业的运行得不到保证。如果我们能为每个作业引入前面所述的动态优先权，并使作业的优先级随着等待时间的增加而以速率a提高，则长作业在等待一定时间后，必然有机会分配到处理机。优先权R变化规律=（等待时间+要求服务时间）/要求服务时间
- 基于时间片的轮转调度算法
	- 时间片轮转法：系统将所有的就绪进程按先来先服务的原则排成一个队列，每次调度时，把CPU分配给队首进程，并令其执行一个时间片。
	- 多级反馈队列调度算法：
		1. 应设置多个就绪队列，并为各个队列赋予不同的优先级。第一个队列的优先级最高，第二个队列次之，其余各队列的优先权逐个降低。该算法赋予各个队列中进程执行时间片的大小也各不相同，在优先权越高的队列中，为每个进程所规定的执行时间片就越小。例如，第二队列的时间片要比第一个队列的时间片长一倍，......，第i+1个队列的时间片要比i个队列的时间片长一倍。
		2. 当一个新进程进入内存后，首先将它放入第一队列的末尾，按FCFS原则排队等待调度。当轮到该进程执行时，如果它能在该时间片内完成，便可准备撤离系统；如果它在一个时间片结束时尚未完成，调度程序便将该进程转入第二队列的末尾。再同样地按FCFS原则等待调度执行；如果它在第二队列中运行一个时间片后仍未完成，再依次将它放入第三队列，......，如此下去，当一个长作业从第一队列依次降到第n队列后，在第n队列便采取按时间片轮转的方式运行。
		3. 仅当第一队列空闲时，调度程序下才调度第二队列中的进程运行；仅当第1~（i-1）队列均空时，才会调度第i队列中的进程运行。如果处理机正在第i队列中为某进程服务时，又有新进程进入优先权较高的队列（第1~（i-1）中的任何一个队列），则此时新进程将抢占正在运行进程的处理机，即由调度程序把正在运行的进程放回到第i队列的末尾，把处理机分配给新到的高优先权进程。再多级反馈队列调度算法中，如果规定第一个队列的时间片略大于多数人机交互所需之处理时间时，便能够较好的满足各种类型用户的需要。

### 6. 线程调度
#### 线程调度算法
- 抢占式调度:指的是每条线程执行的时间、线程的切换都由系统控制
- 协同式调度:指某一线程执行完后主动通知系统切换到另一线程上执行,缺点：如果一个线程编写有问题，运行到一半就一直阻塞，那么可能导致整个系统崩溃。
线程调度器选择优先级最高的线程运行，但是，如果发生以下情况，就会终止线程的运行：
1. 线程体中调用了yield方法让出了对cpu的占用权利
2. 线程体中调用了sleep方法使线程进入睡眠状态
3. 线程由于IO操作受到阻塞
4. 另外一个更高优先级线程出现
5. 在支持时间片的系统中，该线程的时间片用完

## 二）. 计算机网络

### 1. Http和Https的区别
Http协议运行在TCP之上，明文传输，客户端与服务器都无法验证对方的身份；Https是身披SSL（Secure Socket Layer）外壳Http，运行在SSL上，SSL运行在TCP之上，是添加了加密和认证机制的HTTP。Https的加密机制是一种共享密钥加密和公开密钥加密并用的混合加密机制。
#### 主要不同：
- 端口不同：http与https使用的连接方式不同，用的端口也不一样，http是80端口，https是443端口；
- 资源消耗：https由于加减密处理消耗更多的CPU和内存资源；
- 开销：https通信需要证书，而证书一般需要向认证机构购买；

### 2. 对称加密与非对称加密
   对称密钥加密是指加密和解密使用同一个密钥的方式，这种方式存在的最大问题就是密钥发送问题，即如何安全地将密钥发给对方；而非对称加密是指使用一对非对称密钥，即公钥和私钥，公钥可以随意发布，但私钥只有自己知道。发送密文的一方使用对方的公钥进行加密处理，对方接收到加密信息后，使用自己的私钥进行解密。
   由于非对称加密的方式不需要发送用来解密的私钥，所以可以保证安全性；但是和对称加密比起来，它非常的慢，所以我们还是要用对称加密来传送消息，但对称加密所使用的密钥我们可以通过非对称加密的方式发送出去。

### 3. TCP协议保证传输的可靠性
TCP提供一种面向连接的、可靠的字节流服务。其中，面向连接意味着两个使用TCP的应用（通常是一个客户和一个服务器）在彼此交换数据之前必须先建立一个TCP连接。在一个TCP连接中，仅有两方进行彼此通信；而字节流服务意味着两个应用程序通过TCP链接交换8bit字节构成的字节流，TCP不在字节流中插入记录标识符。
#### 对于可靠性，TCP通过以下的方式进行保证：
- 数据包校验：目的是检测数据在传输过程中的任何变化，若校验出包有错，则丢弃报文段并且不会给出响应，这时TCP发送数据端超时后会重发数据；
- 对失效数据包重排序：既然TCP报文段作为IP数据报来传输，而IP数据报的到达可能会失序，因此TCP报文段的到达也可能会失序。TCP将对失序数据进行重新排序，然后才交给应用层；
- 丢弃重复数据：对于重复数据，能够丢弃重复数据；
- 应答机制：当TCP收到发自TCP连接另一端的数据，它将发送一个确认。这个确认不是立即发送，通常将推迟几分之一秒；
- 超时重发：当TCP发出一个段后，它启动一个定时器，等待目的端确认收到这个报文段。如果不能及时收到一个确认，将重发这个报文段；
- 流量控制：TCP连接的每一方都有固定大小的缓冲空间。TCP的接受端只允许另一端发送接受端缓冲区所能接纳的数据，这可以防止较快主机致使较慢主机的缓冲区溢出，这就是流量控制。TCP使用的流量控制协议是可变大小的滑动窗口协议。

### 4. TCP和UDP的区别：
- TCP是面向连接的，UDP是无连接的；
- TCP是可靠的，UDP是不可靠的；
- TCP只支持点对点通信，UDP支持一对一、一对多、多对一、多对多的通信模式；
- TCP是面向字节流的，UDP是面向报文的；
- TCP有拥塞控制机制；UDP没有拥塞控制，适合媒体通信；
- TCP首部开销（20个字节）比UDP的首部开销（8个字节）要大；
- TCP常见应用层协议：FTP(21)， Telnet(23)， SMTP(25), POP3(110), HTTP(80);UDP常见应用层协议：DNS(53)，SNMP(161)，TFTP(69)；

### 5. TCP的拥塞处理
计算机网络中的带宽、交换结点中的缓存及处理机等都是网络的资源。在某段时间，若对网络中某一资源的需求超过了该资源所能提供的可用部分，网络的性能就会变坏，这种情况就叫做拥塞。拥塞控制就是 防止过多的数据注入网络中，这样可以使网络中的路由器或链路不致过载。注意，拥塞控制和流量控制不同，前者是一个全局的过程，而后者指点对点通信量的控制。拥塞控制的方法主要有以下四种：
1. 慢启动：不要一开始就发送大量的数据，先探测一下网络的拥塞程度，也就是说由小到大逐渐增加拥塞窗口的大小；
2. 拥塞避免：拥塞避免算法让拥塞窗口缓慢增长，即每经过一个往返时间RTT就把发送方的拥塞窗口cwnd加1，而不是加倍，这样拥塞窗口按线性规律缓慢增长；
3. 快重传：快重传要求接收方在收到一个失序的报文段后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方）而不要等到自己发送数据时捎带确认。快重传算法规定，发送方只要一连收到三个重复确认就应当立即重传对方尚为收到的报文段，而不必继续等待设置的重传计时器时间到期；
4. 快恢复：快重传配合使用的还有快恢复算法，当发送方连续收到三个重复确认时，就执行“乘法减小”算法，把ssthresh门限减半，但是接下去并不执行慢开始算法，因为如果网络出现拥塞的话就不会收到好几个重复的确认，所以发送方现在认为网络可能没有出现拥塞，所以此时不执行慢开始算法，而是将cwnd设置为ssthresh的大小，然后执行拥塞避免算法。

### 6. Http
#### HTTP请求信息由3部分组成：
1. 请求方法（GET/POST）、URI、协议/版本
2. 请求头(Request Header)：Content-Type、端口号Host、Cookie
3. 请求正文：包含客户提交的查询字符串信息
请求头和请求正文之间是一个空行
#### HTTP响应也由3个部分构成：
1. 状态行：状态代码及描述 如404、500
2. 响应头(Response Header)：Content-Type 、Server、Date
3. 响应正文：html代码
#### HTTP常见状态码
- 200 ：请求成功，成功返回网页
- 301 ：资源（网页等）被永久转移到其它URL
- 302 ：资源（网页等）被临时转移到其它URL
- 304 ：请求未修改、命中缓存
- 401 ：未授权
- 403 ：服务器拒绝请求
- 404 ：请求的网页或资源不存在
- 500 ：内部服务器错误，无法完成请求
- 502 ：错误网关
- 503 ：请求未完成，服务器临时过载或宕机
- 504 ：网关超时

### 7. Http请求方法
1. GET:请求指定的页面信息，并返回实体主体；
2. HEAD：类似于GET请求，不过返回的响应中没有具体的内容，用于获取报头；
3. POST：向指定资源提交数据进行处理请求。数据被包含在请求体中，POST请求可能会导致新的资源的建立或已有资源的修改；
4. PUT:从客户端向服务器传送的数据取代指定的文档的内容；
5. DELETE：请求服务器删除指定的页面；
6. CONNECT：HTTP/1.1协议中预留给能够将连接改为管道方式的代理服务器；
7. OPTIONS：允许客户端查看服务器的性能；
8. TRACE：回显服务器收到的请求，主要用于测试或诊断；

### 8. 从输入网址到获取页面的过程
1. 浏览器查询DNS，获取域名对应的IP地址：具体过程包括浏览器搜索自身的DNS缓存、搜索操作系统的DNS缓存、读取本地的Host文件和向本地DNS服务器进行查询等。对于向本地DNS服务器进行查询，如果要查询的域名包含在本地配置区域的资源中，则返回解析结果给客户机，完成域名解析；如果要查询的域名不由本地DNS服务器区域解析，但该服务器已缓存了此网址映射关系，则调用这个IP地址映射，完成域名解析。如果本地域名服务器并为缓存该网址映射关系，那么将根据其设置发起递归查询或迭代查询；
2. 浏览器获得域名对应的IP地址以后，浏览器向服务器请求建立连接，发起三次握手；
3. TCP/IP连接建立起来后，浏览器向服务器发送http请求；
4. 服务器接收到这个请求，并根据路径参数映射到特定的请求处理器进行处理，并将处理结果及相应的视图返回给浏览器；
5. 浏览器解析并渲染视图，若遇到对js文件、css文件及图片等静态资源的引用，则重复上述步骤并向服务器请求这些资源；
6. 浏览器根据其请求到的资源、数据渲染页面，最终向用户呈现一个完整的页面。

### 9. Session,Cookie与Application
Cookie和Session都是客户端与服务器之间保持状态的解决方案，具体来说，cookie机制采用的是客户端保持状态的方案，而session机制采用的是在服务器端保持状态的方案。
- Cookie及其相关的API：Cookie实际上是一小段的文本信息。客户端请求服务器，如果服务器需要记录该用户状态，就使用response向客户端浏览器颁发一个cookie，而客户端浏览器会把cookie保存起来。当浏览器再请求该网站时，浏览器把请求的网址连同该cookie一同提交给服务器，服务器检查该cookie，以此来辨认用户状态。服务器还可以根据需要修改cookie的内容。
- Session：同样地，会话状态也可以保存在服务器端。客户端请求服务器，如果服务器记录该用户状态，就获取session来保存状态，这时，如果服务器已经为此客户端创建过session，服务器就按照sessionid把这个session检索出来使用；如果客户端请求不包含sessionid，则为次客户端创建一个session并且生成一个与此session相关联的sessionid，并将这个sessionid在本次响应中返回给客户端保存。保存这个sessionid的方式可以采用cookie机制，这样在交互过程中浏览器可以自动的按照规则把这个标识发送给服务器；若浏览器禁用cookie的话，可以用过URL重写机制将sessionid传回服务器;
- Session与Coookie对比：
	- 实现机制：Session的实现常常依赖于cookie机制，通过cookie机制回传sessionid；
	- 大小限制：cookie有大小限制并且浏览器对每个站点也有cookie的个数限制，session没有大小限制，理论上只与服务器的内存大小有关；
	- 安全性：cookie存在安全隐患，通过拦截或本地文件找到cookie后可以进行攻击，而session由于保存在服务器端，相对更加安全；
	- 服务器资源消耗：session是保存在服务器端上会存在一段时间才会消失，如果session过多会增加服务器的压力；
- Application：与一个web应用程序相对应，为应用程序提供了一个全局的状态，所有客户都可以使用该状态；

## 三）. C++
## 四）. Docker
### 1. 什么是docker
Docker是一个容器化平台，它以容器的形式将您的应用程序及其所有依赖项打包在一起，以确保您的应用程序在任何环境中无缝运行。

### 2. docker与虚拟机不不同
Docker不是虚拟化方法。它依赖于实际实现基于容器的虚拟化或操作系统级虚拟化的其他工具。为此，Docker最初使用LXC驱动程序，然后移动到libcontainer现在重命名为runc。Docker主要专注于在应用程序容器内自动部署应用程序。应用程序容器旨在打包和运行单个服务，而系统容器则设计为运行多个进程，如虚拟机。因此，Docker被视为容器化系统上的容器管理或应用程序部署工具。
1. 容器不需要引导操作系统内核，因此可以在不到一秒的时间内创建容器。此功能使基于容器的虚拟化比其他虚拟化方法更加独特和可取。
2. 由于基于容器的虚拟化为主机增加了很少或没有开销，因此基于容器的虚拟化具有接近本机的性能。
3. 对于基于容器的虚拟化，与其他虚拟化不同，不需要其他软件。
4. 主机上的所有容器共享主机的调度程序，从而节省了额外资源的需求。
5. 与虚拟机映像相比，容器状态（Docker或LXC映像）的大小很小，因此容器映像很容易分发。
6. 容器中的资源管理是通过cgroup实现的。Cgroups不允许容器消耗比分配给它们更多的资源。虽然主机的所有资源都在虚拟机中可见，但无法使用。这可以通过在容器和主机上同时运行top或htop来实现。所有环境的输出看起来都很相似。

## 五）. Websocket 
### 1. 什么是websocket
WebSocket是HTML5一种新的协议，WebSocket是真正实现了全双工通信的服务器向客户端推的互联网技术，是一种在单个TCP连接上进行全双工通讯协议。

### 2. http与websocket的区别
http协议是短链接，因为请求之后，都会关闭连接，下次重新请求数据，需要再次打开连接。WebSocket协议是一种长连接，只需要通过一次请求来初始化链接，然后所有的请求和响应都是通过这个TCP链接进行通信。websocket相对与http的头部多了一个upgrade信息。

## 六）. kafka
### 1. 什么是kafka
Kafka是分布式发布-订阅消息系统，它最初是由LinkedIn公司开发的，之后成为Apache项目的一部分，Kafka是一个分布式，可划分的，冗余备份的持久性的日志服务，它主要用于处理流式数据。

### 2.为什么要使用 kafka，为什么要使用消息队列
- 缓冲和削峰：上游数据时有突发流量，下游可能扛不住，或者下游没有足够多的机器来保证冗余，kafka在中间可以起到一个缓冲的作用，把消息暂存在kafka中，下游服务就可以按照自己的节奏进行慢慢处理。
- 解耦和扩展性：项目开始的时候，并不能确定具体需求。消息队列可以作为一个接口层，解耦重要的业务流程。只需要遵守约定，针对数据编程即可获取扩展能力。
- 冗余：可以采用一对多的方式，一个生产者发布消息，可以被多个订阅topic的服务消费到，供多个毫无关联的业务使用。
- 健壮性：消息队列可以堆积请求，所以消费端业务即使短时间死掉，也不会影响主要业务的正常进行。
- 异步通信：很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。


## 七）. 数据库
## 八）. linux
## 九）. algorithm
## 十）. 多线程
c++多线程有几种实现方法
1. 继承 Thread 类
2. 实现 Runnable 接口再 new Thread(YourRunnableOjbect) 

### 1. 并发与并行
并发指的是多个任务交替进行，并行是指真正意义上的“同时进行”，实际上，如果系统内只有一个CPU，使用多线程时，在真实系统环境下不能并行，只能通过切换时间片的方式交替进行，从而并发执行任务。真正的并行只能出现在拥有多个CPU的系统中。
### 2. 什么是协程
协程是一种用户态的轻量级线程，协程的调度完全由用户控制。从技术的角度来说，“协程就是你可以暂停执行的函数”。协程拥有自己的寄存器上下文和栈。协程调度切换时，将寄存器上下文和栈保存到其他地方，在切回来的时候，恢复先前保存的寄存器上下文和栈，直接操作栈则基本没有内核切换的开销，可以不加锁的访问全局变量，所以上下文的切换非常快。
#### 协程与线程的区别
- 一个线程可以多个协程，一个进程也可以单独拥有多个协程。
- 线程进程都是同步机制，而协程则是异步。
- 协程能保留上一次调用时的状态，每次过程重入时，就相当于进入上一次调用的状态。
- 线程是抢占式，而协程是非抢占式的，所以需要用户自己释放使用权来切换到其他协程，因此同一时间其实只有一个协程拥有运行权，相当于单线程的能力。
- 协程并不是取代线程, 而且抽象于线程之上, 线程是被分割的CPU资源, 协程是组织好的代码流程, 协程需要线程来承载运行, 线程是协程的资源, 但协程不会直接使用线程, 协程直接利用的是执行器(Interceptor), 执行器可以关联任意线程或线程池, 可以使当前线程, UI线程, 或新建新程.。
- 线程是协程的资源。协程通过Interceptor来间接使用线程这个资源。

### 3. 什么是线程死锁？如何避免死锁？
多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。由于线程被无限期地阻塞，因此程序不可能正常终止。假如线程 A 持有资源 2，线程 B 持有资源 1，他们同时都想申请对方的资源，所以这两个线程就会互相等待而进入死锁状态。
#### 避免死锁的几个常见方法
- 避免一个线程同时获取多个锁
- 避免一个线程在锁内同时占用多个资源，尽量保证每个锁只占用一个资源
- 尝试使用定时锁
- 对于数据库锁，加锁和解锁必须在一个数据库连接里，否则会出现解锁失败的情况。

### 4. 乐观锁和悲观锁
- 乐观锁：总是假设最好的情况，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号机制和CAS算法实现。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于write_condition机制，其实都是提供的乐观锁。在Java中java.util.concurrent.atomic包下面的原子变量类就是使用了乐观锁的一种实现方式CAS实现的。
- 悲观锁：总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁（共享资源每次只给一个线程使用，其它线程阻塞，用完后再把资源转让给其它线程）。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。Java中synchronized和ReentrantLock等独占锁就是悲观锁思想的实现。
#### 乐观锁常见的两种实现方式
乐观锁一般会使用版本号机制或CAS算法实现
1. 版本号机制:一般是在数据表中加上一个数据版本号version字段，表示数据被修改的次数，当数据被修改时，version值会加一。当线程A要更新数据值时，在读取数据的同时也会读取version值，在提交更新时，若刚才读取到的version值为当前数据库中的version值相等时才更新，否则重试更新操作，直到更新成功。
2. CAS算法：即compare and swap（比较与交换），是一种有名的无锁算法。无锁编程，即不使用锁的情况下实现多线程之间的变量同步，也就是在没有线程被阻塞的情况下实现变量的同步，所以也叫非阻塞同步（Non-blocking Synchronization）。CAS算法涉及到三个操作数
- 需要读写的内存值 V
- 进行比较的值 A
- 拟写入的新值 B

当且仅当 V 的值等于A时，CAS通过原子方式用新值B来更新V的值，否则不会执行任何操作（比较和替换是一个原子操作）。一般情况下是一个自旋操作，即不断的重试。

### 5. 多线程开发带来的问题与解决方法
使用多线程主要会带来以下几个问题：
##### 1. 线程安全问题：
线程安全问题指的是在某一线程从开始访问到结束访问某一数据期间，该数据被其他的线程所修改，那么对于当前线程而言，该线程就发生了线程安全问题，表现形式为数据缺失，数据不一致等。
###### 线程安全问题发生的条件：
1. 多线程环境下，即存在包括自己在内存在有多个线程。
2. 多线程环境下存在共享资源，且多线程操作该共享资源。
3. 多个线程必须对该共享资源有非原子性操作。

###### 线程安全问题的解决思路：
1. 尽量不使用共享变量，将不必要的共享变量变成局部变量来使用。
2. 使用synchronized关键字同步代码块，或者使用jdk包中提供的Lock为操作进行加锁。
3. 使用ThreadLocal为每一个线程建立一个变量的副本，各个线程间独立操作，互不影响。

##### 2.性能问题
线程的生命周期开销是非常大的，一个线程的创建到销毁都会占用大量的内存。同时如果不合理的创建了多个线程，cup的处理器数量小于了线程数量，那么将会有很多的线程被闲置，闲置的线程将会占用大量的内存，为垃圾回收带来很大压力，同时cup在分配线程时还会消耗其性能。
###### 解决思路：
利用线程池，模拟一个池，预先创建有限合理个数的线程放入池中，当需要执行任务时从池中取出空闲的先去执行任务，执行完成后将线程归还到池中，这样就减少了线程的频繁创建和销毁，节省内存开销和减小了垃圾回收的压力。同时因为任务到来时本身线程已经存在，减少了创建线程时间，提高了执行效率，而且合理的创建线程池数量还会使各个线程都处于忙碌状态，提高任务执行效率，线程池还提供了拒绝策略，当任务数量到达某一临界区时，线程池将拒绝任务的进入，保持现有任务的顺利执行，减少池的压力。

##### 3. 活跃性问题
- 死锁，假如线程 A 持有资源 2，线程B持有资源1，他们同时都想申请对方的资源，所以这两个线程就会互相等待而进入死锁状态。多个线程环形占用资源也是一样的会产生死锁问题。想要避免死锁，可以使用无锁函数（cas）或者使用重入锁（ReentrantLock），通过重入锁使线程中断或限时等待可以有效的规避死锁问题。
- 饥饿，饥饿指的是某一线程或多个线程因为某些原因一直获取不到资源，导致程序一直无法执行。如某一线程优先级太低导致一直分配不到资源，或者是某一线程一直占着某种资源不放，导致该线程无法执行等。与死锁相比，饥饿现象还是有可能在一段时间之后恢复执行的。可以设置合适的线程优先级来尽量避免饥饿的产生。
- 活锁，活锁体现了一种谦让的美德，每个线程都想把资源让给对方，但是由于机器“智商”不够，可能会产生一直将资源让来让去，导致资源在两个线程间跳动而无法使某一线程真正的到资源并执行，这就是活锁的问题。

##### 4.阻塞
阻塞是用来形容多线程的问题，几个线程之间共享临界区资源，那么当一个线程占用了临界区资源后，所有需要使用该资源的线程都需要进入该临界区等待，等待会导致线程挂起，一直不能工作，这种情况就是阻塞，如果某一线程一直都不释放资源，将会导致其他所有等待在这个临界区的线程都不能工作。当我们使用synchronized或重入锁时，我们得到的就是阻塞线程，如论是synchronized或者重入锁，都会在试图执行代码前，得到临界区的锁，如果得不到锁，线程将会被挂起等待，知道其他线程执行完成并释放锁且拿到锁为止。
###### 解决思路：
可以通过减少锁持有时间，读写锁分离，减小锁的粒度，锁分离，锁粗化等方式来优化锁的性能。

### 6. 线程池
线程池就是首先创建一些线程，它们的集合称为线程池。使用线程池可以很好地提高性能，线程池在系统启动时即创建大量空闲的线程，程序将一个任务传给线程池，线程池就会启动一条线程来执行这个任务，执行结束以后，该线程并不会死亡，而是再次返回线程池中成为空闲状态，等待执行下一个任务。
#### 线程池的工作机制
在线程池工作模式下，任务会提交给线程池，而不是直接提交给某个线程，线程池在拿到任务之后，就在内部寻找是否有空闲的线程，如果有则将任务交给某个空闲的线程。
#### 使用线程池的原因
多线程运行时间，系统不断的启动和关闭新线程，成本非常高，会过渡消耗系统资源，以及过渡切换线程的危险，从而可能导致系统资源的崩溃。这时，线程池就是最好的选择了。
#### 线程池的好处
- 降低资源消耗。通过重复利用已创建的线程，降低线程创建和销毁造成的消耗。
- 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。
- 提高线程的可管理性。线程是稀缺资源，如果无限制地创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一分配、调优和监控。

#### 几种常见的线程池及适用场景
- FixedThreadPool：可重用固定线程数的线程池。（适用于负载比较重的服务器）
- SingleThreadExecutor：只会创建一个线程执行任务。（适用于需要保证顺序执行各个任务；并且在任意时间点，没有多线程活动的场景。）
- CachedThreadPool：是一个会根据需要调整线程数量的线程池。（大小无界，适用于执行很多的短期异步任务的小程序，或负载较轻的服务器）
- ScheduledThreadPool：继承自ThreadPoolExecutor。它主要用来在给定的延迟之后运行任务，或者定期执行任务。使用DelayQueue作为任务队列。

#### 线程池都有哪几种工作队列
- ArrayBlockingQueue：是一个基于数组结构的有界阻塞队列，此队列按FIFO（先进先出）原则对元素进行排序。
- LinkedBlockingQueue：是一个基于链表结构的阻塞队列，此队列按FIFO排序元素，吞吐量通常要高于ArrayBlockingQueue。静态工厂方法Executors.newFixedThreadPool()使用了这个队列。
- SynchronousQueue：是一个不存储元素的阻塞队列。每个插入操作必须等到另一个线程调用移除操作，否则插入操作一直处于阻塞状态，吞吐量通常要高于Linked-BlockingQueue，静态工厂方法Executors.newCachedThreadPool使用了这个队列。
- PriorityBlockingQueue：一个具有优先级的无限阻塞队列。

#### 线程池执行流程
任务被提交到线程池，会先判断当前线程数量是否小于corePoolSize，如果小于则创建线程来执行提交的任务，否则将任务放入workQueue队列，如果workQueue满了，则判断当前线程数量是否小于maximumPoolSize,如果小于则创建线程执行任务，否则就会调用handler，以表示线程池拒绝接收任务。









    

